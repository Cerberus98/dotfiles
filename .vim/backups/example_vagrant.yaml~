---
# Here is an ascii diagram that matches
# up to the example topology yaml
# definition in this file.
#
# mgmt net: 2.3.4.1/24 ----------|
#                                |
# ---------  AA:BB:CC:DD:EE:00   |
# | node0 |----------------------|
# ---------       10.15.15.2     |
#                                |
#                                |
# ---------  AA:BB:CC:DD:EE:01   |
# | node1 |----------------------|
# ---------       10.15.15.3     |
#                                |
#                                |
# ---------  AA:BB:CC:DD:EE:02   |
# | node2 |----------------------|
# ---------       10.15.15.4     |
#                                |

# Yaml definition of the disk drives available on
# all hosts
# Required
disks:
  # Comma-separated list of drives; note that these
  # will be automatically repartitioned
  drive_list: sda
  # The boot drive to provide the bootloader in kickstart
  boot_drive: sda

# Yaml definition of mgmt network
# This always needs to be supplied
# by the Customer.
networks:
  # the mgmt network provides common configuration for
  # the host interfaces defined below
  mgmt:
    netmask: 255.255.255.0
    nameserver: 8.8.8.8

  # special entry for virtualbox-style management network
  # and used to provide a second interface on each host
  mgmt-vbox:
    bootproto: dhcp
    device: enp0s3

# Vagrant-specific values
vagrant:
  ipxe_iso_location: http://filez.lab.dfj.io/sparta/ipxe-boot.iso

# These global fields apply to all nodes.
# They can be locally overridden by specifying
# the config key:value under the node definition
#
# NOTE: cimc_* information is not required in proxmox
# environments
global:
  deploy_type: vagrant_vbox

  # primary SSH key to use for authentication to the VMs
  ssh_pubkey: ~/.ssh/id_rsa.pub

  # the available IP range for assignment to pods
  cluster_ip_range: 10.254.0.0/16

  # the search domain for service discovery between pods
  cluster_domain: cluster.local

  # the cluster-specific IP address to assign to kube-dns; this must
  # be an unused IP and within the `global.cluster_ip_range`
  cluster_dns: 10.254.0.10

  # The cluster-specifc IP address of the API seerver; this must
  # be the first address in the cluster_ip_range block
  cluster_kube_api: 10.254.0.1

  # nfs mount points - in a vagrant environment
  # the "vagrant" server will set up synced folders
  # using vagrant's NFS method
  nfs_mounts:
    - server: vagrant
      share: ~/.harmony/vagrant/shared
      mountpoint: /var/lib/kolla

# Node definition defines what nodes the control plane
# will be deployed on.
hosts:
    # host_mac is required in proxmox environments and
    # MUST match the MAC addresses defined in the network
    # sections above AND also in your axion configuration
  - host_mac: AA:BB:CC:DD:EE:00

    # host_ip is also required in proxmox environments
    # and will be used to establish static IP address for
    # the master role
    host_ip: 10.15.15.2

    # the hostname used by vagrant, and also to set hostname
    # on the installed Linux
    host_name: node0

    # host_bootif is optional, but highly recommended; this
    # defines the interface to dhcp with during theinitramfs
    # process. Without specifying this all network devices will
    # attempt DHCP and slow down the boot process. By providing
    # this option we will shortcut the DHCP process and booting
    # will be much faster
    host_bootif: enp0s8

    # network interfaces for kickstart
    interfaces:
      - bootproto: static
        device: enp0s8
        ipv6: auto
        ip: 10.15.15.2
        network: mgmt
      - network: mgmt-vbox

    # The list of control plane roles this node will fullfill
    roles:
      # The iso-install node is installed from ISO
      - iso-install
      - node

    # The second (node1) definition starts here
  - host_mac: AA:BB:CC:DD:EE:01
    host_ip: 10.15.15.3
    host_name: node1
    host_bootif: enp0s8

    interfaces:
      - bootproto: static
        device: enp0s8
        ipv6: auto
        ip: 10.15.15.3
        network: mgmt
      - network: mgmt-vbox
    roles:
      # The node role will be netbooted
      # and will run the kubernetes worker services
      - node
    # The third (node2) definition starts here
  - host_mac: AA:BB:CC:DD:EE:02
    host_ip: 10.15.15.4
    host_name: node2
    host_bootif: enp0s8
    interfaces:
      - bootproto: static
        device: enp0s8
        ipv6: auto
        ip: 10.15.15.4
        network: mgmt
      - network: mgmt-vbox
    roles:
      - node

# Required: dnsmasq-specific configuration options
dnsmasq:
  # The range of IP addresses available for lease from dnsmasq
  dhcp_range: 10.15.15.2,10.15.15.5

# Required: pixiecore-server configuration options
pixiecore_server:
  # defaults to containers.cisco.com registry unless docker.insecure_registry is provided
  image: harmony-baremetal/pixiecore-server:0.3.0
  host_port: 9091

# Required: docker auth and registry configuration options
docker:
  auth: aGFybW9ueS1iYXJlbWV0YWwrbGVvbmlkYXM6RzE1VEZFNk40TVNMUkZEUDdLSVdWU1BTWUwxNVhaOTFYMkFWTFdOQ1o5Q0MxTjM4MFQ5QUpHTTZORFJFSVA1Mw==

  secret:
    name: containers.cisco.com
    namespace: default
    dockercfg: eyJodHRwczovL2NvbnRhaW5lcnMuY2lzY28uY29tIjp7InVzZXJuYW1lIjoiaGFybW9ueS1iYXJlbWV0YWwrbGVvbmlkYXMiLCJwYXNzd29yZCI6IkcxNVRGRTZONE1TTFJGRFA3S0lXVlNQU1lMMTVYWjkxWDJBVkxXTkNaOUNDMU4zODBUOUFKR002TkRSRUlQNTMiLCJlbWFpbCI6Imhhcm1vbnktYmFyZW1ldGFsQGNpc2NvLmNvbSIsImF1dGgiOiJhR0Z5Ylc5dWVTMWlZWEpsYldWMFlXd3JiR1Z2Ym1sa1lYTTZSekUxVkVaRk5rNDBUVk5NVWtaRVVEZExTVmRXVTFCVFdVd3hOVmhhT1RGWU1rRldURmRPUTFvNVEwTXhUak00TUZRNVFVcEhUVFpPUkZKRlNWQTFNdz09In19

  # by default all images are mirrored on an insecure registry
  # running in filez; for vagrant users you also have the option of
  # mirroring to a locally running registry which should speed up
  # the node0 boot process. See sparta/registry/README.md for more
  # info on local registry and update the value below to point to
  # your local IP
  insecure_registry: filez.lab.dfj.io:5000

# To enable logging to Zeus uncomment the zeus section below.
# Below is a test bucket you can view at:
# https://app.ciscozeus.io (credentials pinned in #harmony-monitoring)
# Since this will be in use by other, if you plan to use this long term
# you should reach out to #harmony-monitoring to get a dedicated bucket
# You will also need to add this line to the manifest_loader section:
# - /etc/sparta/manifests/monitoring-daemonset.yaml
#zeus:
#    ingestion_point: data03.ciscozeus.io
#    username: harmonymahl
#    token: crumpsponwb3ujjamzy2i5d218myvz6p

# Optional: provides a list of additional manifests to load at run time
manifest_loader:
  manifests:
    - /etc/sparta/manifests/containers.cisco.com-secret.yaml
    - /etc/sparta/manifests/kubedns-svc.yaml
    - /etc/sparta/manifests/kubedns-deployment.yaml
    - /etc/sparta/manifests/tiller-deployment.yaml
    - /etc/sparta/manifests/netboot-deployment.yaml
    - /etc/sparta/manifests/netboot-svc.yaml
    - /etc/sparta/manifests/controller-manager-deploy.yaml
    - /etc/sparta/manifests/scheduler-deploy.yaml
    - /etc/sparta/manifests/package-manager-deployment.yaml

# NOTE:
# docker and bootstrapper configuration is not necessary in proxmox
# environments and can be safely removed

# users to create on the host; optionally assign them to
# groups and configure authorized_keys files
users:
  # username to use for the created user
  admin:
    # groups to assign the user; the wheel group provides
    # passwordless sudo access to this user
    groups:
      - wheel
    # public ssh keys to put into user's authorized_keys file
    authorized_keys:
      - ssh-rsa mypublicsshkeygoeshere== bob@localhost
