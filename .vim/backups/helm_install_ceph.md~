# Cleanup Helm first. Guess what: you can't remove charts without editing files
rm -rf ~/.helm

cd ~/code/python/kargo

vagrant up


# If you have a bad kube context, edit ~/.kube/config and delete the current-config line (or current-context?)
helm init

# Kargo install Ceph
vagrant ssh k8s-01 -c "kubectl get nodes | grep -v NAME | awk '{print \$1}' | xargs -i kubectl label node {} --overwrite ceph-storage=enabled"

# Create the release
cd ~/code/python/kargo/ceph-docker/examples/helm/ceph
./build_all

# Charts mount all volumes as read-only and the first thing the mon does is try to chown things
# Build-all should handle this
# helm create ceph-0.1.0.tgz

# CEPH_PUBLIC_NETWORK needs to be set in the mon deployment to the name of the network used by the OSDs (I assume this is 'bridge' but nfi)

helm install --name ceph-deploy --namespace ceph local/ceph


# ceph-docker branch
# This sha *has* worked: e67ba55c747fbe2002fa1424dd4ced89fd8d0c68
# some branch
remotes/origin/build-master-kraken-ubuntu-16.04



# After the ceph deployment is installed, we need to create secrets so the other namespaces can access ceph
# This bash function works. The one in the wiki expects jq to be installed on the system, which kargo doesn't provide
ceph_activate_namespace() {
  kube_namespace=$1
  {
  cat <<EOF
apiVersion: v1
kind: Secret
metadata:
  name: "pvc-ceph-client-key"
type: kubernetes.io/rbd
data:
  key: |
    $(kubectl get secret pvc-ceph-conf-combined-storageclass --namespace=ceph -o json | python -c "import sys,json; x=json.load(sys.stdin); print x['data']['key']")
EOF
  } | kubectl create --namespace ${kube_namespace} -f -
}

# Default is created... by default, and ceph should already be functional
ceph_activate_namespace kube-system

vagrant ssh k8s-01 -c "kubectl get nodes | grep -v NAME | awk '{print \$1}' | xargs -i ceph_activate_namespace {}"

# Not that this is useful, but to hit etcd inside the cluster:
# The best part is that 2379 and 4001 are default ports. If you don't specify --endpoints it will complain that 4001 is inaccessible. The real problem
# is that the default scheme is http, not https. Fucking garbage
etcdctl --endpoints https://127.0.0.1:2379

# BECAUSE YOU DON'T WANT TO USE FUCKING ETCDCTL WITHOUT ALSO TELLING IT TO USE API v3

# This gets all keys forever
ETCDCTL_API=3 etcdctl --endpoints https://127.0.0.1:2379 get "" --prefix=True

# Things are of the form /registry/<k8s type>/<namespace>/<name>
# This gets all the info under the key for the ceph-osd daemonset
ETCDCTL_API=3 etcdctl --endpoints https://127.0.0.1:2379 get /registry/daemonsets/ceph/ceph-osd

# So, if you can't delete your fucking daemonset:
ETCDCTL_API=3 etcdctl --endpoints https://127.0.0.1:2379 del /registry/daemonsets/ceph/ceph-osd

# if things get stuck and won't redeploy, try deleting all the replica sets
kubectl delete $(kubectl get rs -oname  -n kube-system) -n kube-system

# If you need to remove Tiller:
helm reset

# deleting a release. Releases are a helm word for "Shit that's deployed as a unit"
# get the release name. Hopefully there's a better way because this is garbage:
kubectl describe pods ceph-mon-0 -n ceph | grep release=

helm delete <release name>


# To dump a pod to yaml and edit it
kubectl get pod ceph-mon-0 -o yaml -n ceph

# then delete the other one
kubectl delete pod ceph-mon-0 -n ceph

# Then make a new one
kubectl create -f ceph-mon -n ceph

# We also need to edit the ceph networks in values.yaml to include the correct subnets
# Kargo creates 10.233.0.0/16 and that's where the ceph pods seem to land. I need to go figure out why

# And so, from the perspective of the helm chart, it just stands up the ceph cluster. It does *not* stand up any persistentvolumes or persistentvolumeclaims. Why? Because it itself
# isn't intended to use the volumes, duh. You need to create volumes using the existing storage class and then have pods claim them. So, we need to create secrets for the other
# namespaces uses the above secret generation bash script, and then we can create PVs and PVCs for pods in those namespaces.

# So now, the plan is to create a PV, PVC and then stand up a busybox pod or something that uses that ceph claim.
# Doesn't look like we need to create a PV. PVCs can dynamically create them

kind: PersistentVolumeClaim
apiVersion: v1
metadata:
  name: test-claim
  annotations:
    volume.beta.kubernetes.io/storage-class: general
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 2Gi


# Something can't modprobe rbd. The host node can. Looks like the issue could be:
# https://github.com/kubernetes/kubernetes/issues/23924
# kolla-k8s actually mounts /lib/modules on a ceph-rbd-pod which is telling IMO
