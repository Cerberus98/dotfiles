Info Roundup:

We need a service that interfaces with SFDC (Salesforce.com)
I'm still waiting on credentials to play around in the cisco sandbox we've been assigned
Said sandbox can be found at https://cs44.salesforce.com

We need to identify viable key/value stores
  Redis
  etcd
  We could go straight to an object store type db:
    Riak
    Cassandra

The service will *somehow* consume from SFDC, stick the pertinent data into our undefined k/v store, and then generate entitlements from that data. The preferred mechanism is via x.509 certs.


Invidual work items:

Registration Epic:
=======================================
Name this piece
Investigate Salesforce API Clients
Investigate Consumption of Salesforce API
  # Some kind of callback/event system or do we have to poll?

# Individually these would all be a 1 point, except they're probably all doable in a single day
REST API Call: Fetching an Availability Zone from SFDC
REST API Call: Fetching storage records
REST API Call: Fetching socket records
REST API Call: Adding an Availability Zone to SFDC
REST API Call: Increasing or Decreasing Sockets entitlement
REST API Call: Increasing or Decreasing Storage entitlement
REST API Call: Changing sets of attributes around availability zones (Name, for example)
REST API Call: Changing the number of sockets (add or decrease)
REST API Call: Change the amount of storage
REST API Call: Deleting sockets (What does this mean?)
REST API Call: Deleting storage (what does this mean?)
Resource provisioning for registration service
Automation and packaging for registration service

Entitlements Epic:
=======================================
Name this piece
Generating entitlements from the data we receive from Salesforce
  # (how do I generate an x.509 storage cert representing 2TB of storage?)
  # This one probably ends up being a spike due to open questions
  # Some options here are using the extension piece in x509 certs or a signed JSON blob
Entitlement REST API
  # Presentation and delivery of entitlements
  # This one breaks down further

Workflow/orchestration component
======================================
Identify a suitable message bus solution (SPIKE)
  # RabbitMQ still seems like quickest/safest bet

First cut, it's probably just a message bus. Stick a queue service up somewhere and directly react to the messages coming in
What does this look like?
  # Business workflows, as in https://github.com/knipknap/SpiffWorkflow/wiki from http://www.workflowpatterns.com/patterns/control/index.php


First cut basic flow:
======================================
# In this flow, the message bus is somewhat superfluous. It's more about establishing the centralized piece as asynchronous and queue driven
# so each of the individual pieces are built with respect to that logic.

Registration piece consumes SFDC -> push (AZ, storage, socket, etc) changes into k/v as registrations
Registration drops an en event in the messasge bus service.
Message bus workers pop things off queues and:
  1) Pings the entitlemt service over REST
  2) Sends emails to configured email addresses
Entitlement service records Entitlements (BUT DOES NOT GENERATE THEM)

# Alternate. This flow is a little better but stuff is in kind of a shaky state while in the queue
Reg consumes SFDC -> push (...) onto bus, bus pops and pushes to entitlement system && sends an email to ops, entitlement writes things into k/v


IPXE Epic
======================================
IPXE REST API
  https://gist.github.com/ChristopherMacGown/e975507e6cc2e21ca16c5d36e9d000e8
  # Probably just jinja templates + sub-templates on a flask app
  # Should be reactionary

Resource provisioning for deployment of ipxe service
Packaging and automation of ipxe service

Generate a basic IPXE installer for orbital to consume
It has to inject the entitlement certificates
http://ipxe.org/crypto
We've already got the API https://gist.github.com/ChristopherMacGown/e975507e6cc2e21ca16c5d36e9d000e8


Integration Testing
======================================
What does this look like?
Performs all the major actions of the MVP system
Where does it run? Can we have a dedicated place to automatically build, deploy, run and test?
Name it and give it it's own repo
Add it to Jenkins so it's using it to build and run against the celestial stack


Long Term Workflow Engine
======================================
Gut reaction: Pattern matching mapped to actions:
  "Registration" event - "^Registraton.*" => action_call_entitlement
    Each event could have a before_call, call and after_call event
    Could also emulate rabbit and optionally do casts

  Regexs of course don't scale super well, so maybe another pattern matcher
  Events could be pushed into a queue, then we could have N workflow workers and they all just eat from the queue
This also looks like the predicate solver code


# Examples are things like Moxie or Heat
[18:37:22]  <_0x44>	That presumes a workflow engine exists and that that SFDC consumer pushes workflow events into
Functional/integration testing required from the get go on this one


Common Epic:
=======================================
Identifying a k/v store (if not consul)
Resource provisioning for testing the k/v store
  # if not consul or if not sharing the same consul instance with Orbital services)
Testing that k/v store
  # Can probably skip this step if sharing Consul
Automation/packaging for the k/v store
  # Can skip if sharing Consul
Automation/packaging for the SFDC consumer piece
Monitoring and notifications for entitlement service
Monitoring and notifications for SFDC aggregator

Open questions:

Any preferred k/v stores?
  # Probably Consul!
If we use consul, do we use the same instance Orbital is relying on?
  # We will not. The orbital consul is intra-az, whereas ours will live inside the az itself
What does deleting a socket with a socket_id mean?
What does deleting a deleting storage with a storage_id mean?
POST "adds" or "decreases" storage ("inserts" storage is also referencing) but we can PATCH existing ones later, what does that mean? Why do I want to do one versus the other?
Assuming I can both create new storages/AZs/sockets as well as update them, is there any kind of quota system on top of any of this or do we own that too?
What does the registration mechanism look like? Is it polling? will there be an update to it to tell it to pull?
  # Hopefully there's webhooks/events in the salesforce API
  # Otherwise, straight polling interval, I suppose
How do I generate a certificate representing these entitlements?
  # There isn't any precedent in Piston code for doing this
  # Either using the x509 extension stuff or a signed json blob (see Notes below)
Generate a self-signed cert or acquire one? (probably the former, if past entitlement stuff is any indication)
Are we even allowed to use Cisco's OID?
How are these services distributed globally? Are they even?
  Has this been considered in terms of the existing Consul work being done? Is there any DC/regional subdivision of records?
What about monitoring of the service or services + k/v store?
  # [18:29:24]  <_0x44>	You're on the hook for providing the Ops team with what they need _to_ monitor, as far as I'm aware

Notes:

Python can generate x509 certs with the OpenSSL.crypto -> https://gist.github.com/ril3y/1165038
  https://cryptography.io/en/latest/x509/reference/#x-509-extensions
  https://cryptography.io/en/latest/hazmat/primitives/asymmetric/interfaces/
https://the.randomengineer.com/2014/04/18/adding-custom-data-to-x-509-ssl-certificates/
http://smurf.piston.cc/dev-certs/ and http://smurf.piston.cc/dev-entitlements/ hold some clues to the old way
On generating resource specific entitlements:
  "The plan was that we'd put new keys in the CSR and then sign with the relevant CAs"
  "So the entitlement system would generate a CSR with some (cert key) "foo" and the FOO CA would sign to validate"
  [17:06:45]  <_cerberus_>	So if I'm following along, we'd have storage and socket CAs
  [17:07:03]  <_cerberus_>	and then any other resource as necessary
  [17:07:12]  <_0x44>	Yes, and we'd configure openssl with custom OIDs associated with those feature CAs
  [17:07:40]  <_0x44>	https://the.randomengineer.com/2014/04/18/adding-custom-data-to-x-509-ssl-certificates/
  [17:07:43]  <_0x44>	Like that ^
  [17:12:36]  <_0x44>	WORST case scenario, we make the entitlement a 3-tuple of (AZ-cert,AZ-private-key,AZ-metadata.json)
  [17:12:56]  <_0x44>	And validate that AZ-metadata.json was signed by the feature CA chain
  [17:13:51]  <_cerberus_>	Doesn't that end up being functionally equivalent to actually stuffing it in the cert extension?
  [17:14:15]  <_0x44>	Functionally equivalent, but easier
  [17:14:18]  <_cerberus_>	and subsequently is quite a bit easie.
  [17:14:19]  <_cerberus_>	yes
  [17:14:33]  <_0x44>	Not as cool though.
  [17:14:34]  <_0x44>	;)
  [17:14:37]  <_cerberus_>	hahaha, ok
  [17:14:38]  <_cerberus_>	noted
  [17:15:01]  <_0x44>	It'd actually be: (AZ-cert, AZ-key, AZ-metadata.json, AZ-metadata.json.signed)
  [17:22:08]  <_0x44>	Anything in the cryptography.hazmat module, I'd prefer not be delegated to a Jr
